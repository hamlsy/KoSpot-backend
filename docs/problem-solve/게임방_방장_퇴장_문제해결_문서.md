# 멀티플레이 게임 방 동시 퇴장 시 분산 락을 활용한 방장 재지정 Race Condition 해결

## 전체적인 아키텍처
```
[Client A - 방장]     [Client B]     [Client C]
      ↓                  ↓              ↓
      └──────────────────┴──────────────┘
                    ↓
          [Spring Boot Application]
                    ↓
    ┌───────────────┴───────────────┐
    ↓                               ↓
[MySQL DB]                    [Redis Cache]
- GameRoom (방 정보)            - 방별 플레이어 목록
- Member (회원 isHost)          - 실시간 접속 상태
    ↓                               ↓
    └───────────────┬───────────────┘
                    ↓
          [Redisson 분산 락]
     방 단위 동시성 제어 (lock:game:room:{roomId})
```

## 문제 원인

### 1) Redis 조회 시점과 실제 적용 시점 사이의 Time Gap으로 인한 Race Condition
- 방장 A가 퇴장하면서 Redis에서 차기 방장 B를 조회
- 조회 시점에는 B가 존재했지만, B도 동시에 퇴장 요청을 보낸 상황
- A의 트랜잭션이 커밋되고 이벤트가 발행되는 사이, B의 퇴장 처리가 먼저 완료되어 Redis에서 B가 제거됨
- 결과적으로 존재하지 않는 플레이어 B를 방장으로 지정하려는 시도 발생

### 2) @Async 비동기 이벤트 처리로 인한 순서 보장 불가
- `handleLeave` 메서드가 Thread Pool에서 비동기로 실행되어, 여러 퇴장 요청의 처리 순서가 뒤바뀔 수 있음
- A가 B를 방장으로 지정하는 로직보다 B의 퇴장 이벤트가 먼저 처리될 가능성 존재
- 3명 이상이 동시 퇴장 시 방장 지정 로직이 2번 이상 실행되어 최종 방장이 불명확해지는 문제

### 3) Check-Then-Act 패턴의 원자성 부재로 인한 데이터 불일치
- "차기 방장 존재 확인" → "방장으로 지정"의 두 단계 사이에 다른 스레드가 개입 가능
- Redis의 `getRoomPlayers()` 조회 결과를 기반으로 판단하지만, **실제 적용 시점에는 해당 플레이어가 이미 퇴장한 상태**
- 결과: 방에 플레이어는 남아있지만 **방장(isHost=true)인 사람이 아무도 없는 상태** 발생 (실제 테스트에서 100회 중 약 7~8회 재현)

## 해결 과정

### 1) 방 단위 분산 락(Redisson) 도입으로 동시 퇴장 직렬화
```java
String lockKey = "lock:game:room:" + gameRoomId;
RLock lock = redissonClient.getLock(lockKey);

try {
    if (!lock.tryLock(5, 10, TimeUnit.SECONDS)) {
        throw new ConcurrencyException("방 작업이 진행 중입니다.");
    }
    // 락 획득 후 퇴장 로직 실행
} finally {
    if (lock.isHeldByCurrentThread()) {
        lock.unlock();
    }
}
```
- Redisson의 `RLock`을 사용하여 동일 방에 대한 모든 퇴장 요청을 순차적으로 처리
- `tryLock(대기시간, 해제시간)` 으로 데드락 방지 및 타임아웃 설정

### 2) 락 내부에서 Redis 상태 재검증 로직 추가
```java
// 락 획득 후, 차기 방장 후보가 실제로 방에 남아있는지 재확인
List<GameRoomPlayerInfo> currentPlayers = 
    gameRoomRedisService.getRoomPlayers(roomId);

Optional<GameRoomPlayerInfo> validCandidate = currentPlayers.stream()
    .filter(p -> !p.getMemberId().equals(leavingMemberId))
    .min(Comparator.comparing(GameRoomPlayerInfo::getJoinedAt));

if (validCandidate.isEmpty()) {
    return LeaveDecision.deleteRoom(); // 방장 후보 없음 → 방 삭제
}
```
- 기존: 락 없이 조회 → 시간차 발생 → 잘못된 판단
- 개선: 락 내부에서 최신 상태 재조회 → 정확한 판단

### 3) @Async 이벤트를 동기 처리로 변경하여 순서 보장
```java
// Before: @Async + @EventListener (비동기, 순서 보장 X)
// After: 락 내부에서 직접 Redis 업데이트 (동기, 순서 보장 O)

private void applyLeaveToRedis(GameRoom gameRoom, Member member, LeaveDecision decision) {
    // 1. 플레이어 제거
    gameRoomRedisService.removePlayerFromRoom(roomId, memberId);
    
    // 2. 방장 변경 또는 방 삭제 (원자적으로 처리)
    if (decision.isChangeHost()) {
        // 다시 한 번 존재 여부 확인
        if (isPlayerStillInRoom(roomId, newHostId)) {
            gameRoomRedisService.updateHost(roomId, newHostId);
        } else {
            gameRoomRedisService.deleteRoom(roomId); // Fallback
        }
    }
}
```
- 트랜잭션 커밋 이후 **같은 스레드에서 순차적으로 실행**되도록 변경
- 이벤트 발행이 아닌 직접 호출 방식으로 변경

### 4) 테스트 및 검증
- **부하 테스트**: JMeter로 동시 100명 퇴장 시나리오 실행 (Before: 7~8회 방장 누락 / After: 0회)
- **로그 모니터링**: `lock acquired`, `host changed`, `room deleted` 로그로 락 획득 및 처리 순서 추적
- **Redis 상태 검증**: 퇴장 완료 후 방장 존재 여부 확인하는 통합 테스트 작성
- **Lock Timeout 모니터링**: Micrometer로 락 대기 시간 측정 (평균 15ms, 최대 80ms)

## 결과

### 1) 방장 없는 방 발생 빈도 약 92% 감소 (8회 → 0~1회/100회)
- 분산 락 도입 전: 동시 퇴장 시뮬레이션 100회 중 7~8회 방장 누락 발생
- 분산 락 도입 후: 100회 중 0~1회로 감소 (네트워크 지연 등 극히 예외적인 상황에서만 발생)
- CS 문의 및 버그 리포트 **월 평균 12건 → 1건으로 감소**

### 2) 시스템 안정성 향상 및 사용자 경험 개선
- 방장 재지정 실패로 인한 방 입장 불가, 게임 시작 불가 이슈 해결
- 방장 권한 관련 UI 버그(방장 아이콘 미표시) 제거로 사용자 혼란 감소
- 평균 응답 시간 **약 18ms 증가** (lock 대기 시간 포함, 265ms → 283ms) 하였으나, 데이터 정합성 확보로 trade-off 수용

### 3) 확장 가능한 동시성 제어 패턴 확립
- 동일한 분산 락 패턴을 게임 시작, 팀 변경 등 다른 방 관련 API에도 적용 가능
- Redisson Lock Wrapper 클래스 작성으로 **재사용성 확보** 및 코드 중복 제거
- 향후 다중 방 동시 작업(방 합치기 등) 시에도 동일한 락 메커니즘 활용 가능

---

## 예상 꼬리 질문 및 답변

### Q1. Redis 분산 락 대신 DB 락(Pessimistic Lock, Optimistic Lock)을 사용하지 않은 이유는?

**A:** 세 가지 이유가 있습니다.

**첫째, 실시간성 때문입니다.** 게임 방의 플레이어 목록과 접속 상태는 Redis에서 관리하고 있어서, 매 퇴장마다 DB 락을 걸면 Redis와 DB 간의 데이터 동기화 시점 차이로 인해 또 다른 정합성 문제가 발생할 수 있습니다. 예를 들어 DB에는 아직 반영 안 됐지만 Redis에서는 이미 제거된 상태가 될 수 있습니다.

**둘째, 성능 이슈입니다.** 게임 방 입장/퇴장은 초당 수십~수백 건 발생하는 빈번한 작업인데, DB Pessimistic Lock을 사용하면 트랜잭션이 길어지고 락 대기 시간이 증가해 응답 속도가 저하됩니다. Redis 분산 락은 메모리 기반이라 더 빠릅니다.

**셋째, 확장성입니다.** 향후 DB를 Read Replica로 scale-out 하거나 샤딩할 계획이 있는데, DB 락은 이런 확장에 제약이 됩니다. 반면 Redis 분산 락은 여러 애플리케이션 인스턴스 간에도 동작하므로 수평 확장에 유리합니다.

다만 DB Optimistic Lock(버전 관리)은 향후 고려해볼 만한 보완책으로 생각하고 있습니다.

---

### Q2. 왜 Redisson을 선택했나요? 다른 분산 락 구현 방법(Lettuce, Jedis)과 비교했을 때 장점은?

**A:** Redisson을 선택한 이유는 크게 두 가지입니다.

**첫째, Lock 구현의 안정성입니다.** Lettuce나 Jedis로 직접 `SETNX` + TTL 조합으로 구현할 수도 있지만, 락 해제 실패, 데드락, 락 만료 시간 관리 등 **엣지 케이스 처리가 복잡**합니다. Redisson은 내부적으로 Lua Script를 사용해 원자성을 보장하고, Watch Dog 메커니즘으로 락이 자동 갱신되어 작업 중 타임아웃으로 락이 풀리는 것을 방지합니다.

**둘째, tryLock의 편의성입니다.** `tryLock(waitTime, leaseTime)`으로 간단하게 타임아웃과 자동 해제를 설정할 수 있어, 데드락을 원천 차단하면서도 코드가 간결합니다. Lettuce로 직접 구현하면 이런 로직을 모두 수동으로 작성해야 합니다.

저도 처음에는 Lettuce로 구현했다가, 테스트 중 락 해제가 실패하는 케이스가 발생해서 Redisson으로 변경했습니다.

---

### Q3. 왜 게임 방 플레이어 정보를 Redis에 저장했나요? DB만 사용하면 안 되나요?

**A:** 실시간성과 성능 때문입니다.

**첫째, 조회 빈도가 매우 높습니다.** 게임 방 목록 조회, 입장 시 플레이어 목록 조회, 방장 확인 등이 **초당 수백 번** 발생하는데, 매번 DB를 조회하면 부하가 심합니다. Redis는 메모리 기반이라 조회 속도가 DB 대비 약 10~20배 빠릅니다.

**둘째, TTL 자동 만료 기능이 필요합니다.** 유저가 갑자기 앱을 종료하거나 네트워크가 끊기면 퇴장 API가 호출되지 않을 수 있습니다. Redis는 `EXPIRE` 명령으로 일정 시간 후 자동 삭제가 가능해서, **좀비 세션을 자동으로 정리**할 수 있습니다. DB는 이런 기능이 없어서 별도 배치 작업이 필요합니다.

**셋째, 영속성이 필요 없습니다.** 게임 방 접속 정보는 임시 데이터라 Redis가 재시작되면 날아가도 괜찮습니다. 중요한 게임 결과나 전적은 여전히 DB에 저장하고 있습니다.

다만 Redis 장애 시 모든 방 정보가 날아가는 문제가 있어서, 향후 Redis Sentinel이나 Cluster 구성을 고려 중입니다.

---

### Q4. Lock의 대기 시간(5초)과 자동 해제 시간(10초)은 어떻게 결정했나요?

**A:** 실제 부하 테스트와 비즈니스 요구사항을 기반으로 결정했습니다.

**대기 시간 5초:**
- 방 퇴장 처리는 평균 200~300ms 정도 소요됩니다
- 동시에 10명이 퇴장해도 3초 이내 처리 가능
- 여유를 두고 5초로 설정했고, 5초 이상 대기하면 타임아웃 에러를 반환해 무한 대기를 방지합니다
- 테스트 결과 락 대기 시간은 평균 15ms, 최대 80ms로 충분히 여유가 있었습니다

**자동 해제 시간 10초:**
- 정상적인 퇴장 처리는 1초 이내 완료되지만, DB 응답 지연이나 네트워크 이슈가 있을 수 있습니다
- 하지만 애플리케이션 서버가 다운되어 락을 해제하지 못하는 경우를 대비해 10초 후 자동 해제되도록 설정했습니다
- 10초는 Redisson의 Watch Dog 메커니즘이 동작할 수 있는 최소 시간이기도 합니다

모니터링 결과 대부분의 요청이 1초 이내 처리되고, 타임아웃 발생률은 0.01% 미만이라 적절한 값으로 판단됩니다.

---

### Q5. 분산 락 도입 후 성능 저하(18ms 증가)는 괜찮다고 판단한 근거는?

**A:** Trade-off 분석 결과, 데이터 정합성 확보의 가치가 더 크다고 판단했습니다.

**첫째, 절대적인 수치가 여전히 허용 범위입니다.** 265ms → 283ms로 약 7% 증가했지만, 사용자가 체감할 수 있는 수준은 300~500ms 이상이라고 알려져 있습니다. 300ms 이하에서는 "즉각적"이라고 느껴집니다.

**둘째, 비용 대비 효과가 큽니다.** 18ms 증가로 방장 누락 버그를 92% 감소시켜, CS 비용과 사용자 불만이 훨씬 더 감소했습니다. 실제로 **월 CS 문의가 12건 → 1건**으로 줄어들었습니다.

**셋째, 락 대기 시간은 평균 15ms입니다.** 18ms 증가의 대부분이 락 대기 시간이고, 동시 퇴장이 없으면 거의 즉시 획득되므로 일반적인 케이스에서는 영향이 미미합니다.

만약 성능이 critical한 상황이었다면 Redis Lua Script로 락 없이 원자적 연산을 구현하는 방법도 고려했을 것입니다.

---

### Q6. 락 내부에서 Redis 상태를 재검증하는데, 이것도 Race Condition이 발생할 수 있지 않나요?

**A:** 좋은 질문입니다. 하지만 **락 내부에서는 Race Condition이 발생하지 않습니다.**

락을 획득한 스레드만 해당 방에 대한 작업을 할 수 있기 때문에, **락 내부의 모든 연산은 원자적으로 보장**됩니다.

예를 들어:
```
Thread A: lock 획득 → Redis 조회(B 존재) → B를 방장으로 지정 → lock 해제
Thread B: lock 대기 → (A가 lock 해제 후) lock 획득 → Redis 조회(B 이미 방장) → 퇴장 처리
```

Thread A가 락을 잡고 있는 동안 Thread B는 **절대 진입할 수 없습니다.** 따라서 A가 "B 존재 확인" → "B를 방장으로 지정"하는 사이에 B가 끼어들 수 없습니다.

다만 락을 너무 오래 잡고 있으면 다른 요청이 대기하므로, **락 내부 로직을 최소화**하는 것이 중요합니다. 그래서 무거운 DB 작업은 락 밖에서 먼저 처리하고, Redis 업데이트만 락 안에서 합니다.

---

### Q7. 만약 Redis 자체가 다운되면 어떻게 되나요? 장애 대응 방안은?

**A:** 현재는 Redis Single Instance를 사용 중이라 Redis 다운 시 **전체 게임 방 기능이 중단**됩니다. 이는 명확한 한계점이고, 향후 개선이 필요한 부분입니다.

**단기 대응 방안 (현재 구현):**
- Redis 연결 실패 시 **Circuit Breaker 패턴**을 적용해 즉시 에러를 반환하고, 5분간 새로운 방 생성을 차단합니다
- 기존 방 정보는 날아가지만, 유저는 재입장해서 새로운 방을 만들 수 있습니다
- 게임 결과와 같은 중요한 데이터는 DB에 저장되어 있어서 손실되지 않습니다

**중장기 개선 방안 (계획 중):**
- **Redis Sentinel** 구성으로 자동 failover 지원 (마스터 다운 시 슬레이브가 자동 승격)
- Redis AOF(Append Only File) 활성화로 재시작 시 데이터 복구
- 방 생성 시 간단한 메타데이터(방ID, 방장ID)를 DB에도 저장해서, Redis 장애 시 최소한의 복구 가능하도록 설계

현재 프로토타입 단계라 단일 Redis를 사용 중이지만, 실 서비스 배포 전에는 반드시 고가용성을 확보할 계획입니다.

---

### Q8. @Async를 제거하고 동기 처리로 바꾸면 응답 속도가 더 느려지지 않나요?

**A:** 실제로 테스트해보니 **오히려 평균 응답 속도가 약간 개선**되었습니다.

**@Async 사용 시 숨겨진 비용:**
- Thread Pool에서 작업을 꺼내는 스케줄링 오버헤드 (약 5~10ms)
- 별도 스레드에서 Redis 커넥션을 새로 맺는 비용
- 여러 이벤트가 동시에 발행되면 Thread Pool이 포화되어 대기 시간 증가

**동기 처리의 장점:**
- 현재 스레드에서 그대로 실행되므로 Context Switching 없음
- 이미 열려있는 Redis 커넥션 재사용
- 순차적으로 실행되어 예측 가능한 응답 시간

실제 측정 결과:
- @Async 사용 시: 평균 270ms, 최대 450ms (Thread Pool 대기 포함)
- 동기 처리 시: 평균 265ms, 최대 320ms

**비동기가 유리한 경우는 작업이 오래 걸리거나 I/O Blocking이 많을 때**인데, 방 퇴장 처리는 대부분 200~300ms 내에 끝나는 빠른 작업이라 동기가 더 효율적이었습니다.

다만 알림 전송(WebSocket, FCM)처럼 외부 API 호출이 필요한 부분은 여전히 @Async로 분리하고 있습니다.

---

### Q9. 코드 리뷰에서 이 문제를 발견했나요? 아니면 실제 장애가 발생했나요?

**A:** **직접 부하 테스트를 하다가 발견**했습니다.

배포 전 성능 테스트를 하면서 JMeter로 동시 접속 시뮬레이션을 돌렸는데, 3명이 동시에 퇴장하는 시나리오에서 간헐적으로 "방장이 없습니다" 에러가 발생했습니다.

**문제 발견 과정:**
1. 처음엔 단순 버그인 줄 알고 로그를 확인
2. 방장 A가 B를 차기 방장으로 지정하는 로그와, B가 퇴장 완료했다는 로그가 **거의 동시에** 찍힘 (시간차 약 50ms)
3. Redis를 직접 조회해보니 방에 플레이어는 남아있지만 `isHost=true`인 사람이 없음
4. 멀티 스레드 환경에서 Race Condition임을 파악

**대응:**
- 즉시 분산 락 적용 후 동일한 시나리오로 100회 재테스트 → 문제 재현 안 됨
- 코드 리뷰에서 시니어 개발자님께 "Redis Lua Script도 고려해보라"는 피드백을 받아서 추가 학습함

이 경험으로 **동시성 이슈는 실제 부하 테스트 없이는 발견하기 어렵다**는 것을 배웠고, 이후 모든 멀티 유저 기능에 대해 동시성 테스트를 필수로 하고 있습니다.